<!DOCTYPE HTML>

<html>
	<head>
		<style>
			* {
			box-sizing: border-box;
			}

			.container {
			max-width: 100%;
			}

			.one-half {
			float: left;
			width: 55%;
			/* border: 1px dashed red; */
			}

			.other-half {
			float: right;
			width: 40%;
			/* border: 1px dashed red; */
			}

			.other-half img  {
			width: 95%;
			}
			.square {
            background-color: #F2F3F7; /* Set background color of the square */
			border-radius: 15px;
			
        }

		</style>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Chitralekha Gupta</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<div class="author-img" style="background-image: url(https://chitralekha18.github.io/home/papers/chitra.png);"></div>
				<h1 id="colorlib-logo"><a href="index.html">Chitralekha Gupta</a></h1>
				<span class="position">Senior Research Fellow,<br>Founder of <a href="https://musigpro.com">MuSigPro Pte. Ltd.</a> <br>in Singapore</span>
			</div>
			<nav id="colorlib-main-menu" role="navigation" class="navbar">
				<div id="navbar" class="collapse">
					<ul>
						<!-- <li class="active"><a href="#" data-nav-section="home">Home</a></li> -->
						<li><a href="#" data-nav-section="about">About</a></li>
<!--						<li><a href="#" data-nav-section="services">Services</a></li>-->
						<li><a href="#" data-nav-section="publications">Publications</a></li>
						<!-- <li><a href="#" data-nav-section="news">News</a></li> -->
<!--
						<li><a href="#" data-nav-section="education">Education</a></li>
						<li><a href="#" data-nav-section="experience">Work Experience</a></li>
						
-->
						<li><a href="#" data-nav-section="ongoing projects">Ongoing Projects</a></li>
						<li><a href="#" data-nav-section="demos">Demos</a></li>
<!--
						<li><a href="#" data-nav-section="activities">Other Activities</a></li>
						<li><a href="#" data-nav-section="contact">Contact</a></li>
-->
					</ul>
				</div>
			</nav>

			<!-- <div class="colorlib-footer"> -->
				<!-- <p><small>&copy; Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
<!-- Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | Website Template from <a href="https://colorlib.com" target="_blank">Colorlib</a> -->
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --> </span> </small></p>
				<!-- <ul>
					<li><a href="#"><i class="icon-facebook2"></i></a></li>
					<li><a href="#"><i class="icon-twitter2"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>
					<li><a href="#"><i class="icon-linkedin2"></i></a></li>
				</ul> -->
			<!-- </div> -->

		</aside>

		<div id="colorlib-main">
			<br><br>
			<section data-section="about">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<h2>About Me</h2>
                                        I am a Senior Research Fellow at the School of Computing in the National University of Singapore. My key research interest is in human-centered AI modeling for real-world applications. My interests lie at the intersection of generative models, HCI, and audio, particularly in assistive technologies, and controllability of generative models.
										
										<!-- I am currently working in the Augmented Human Lab with A/Prof Suranga Nanayakkara on assistive technologies. In my previous post-doc position, I worked with A/Prof Lonce Wyse on audio generative models. In my first post-doc position with Prof Haizhou Li, I worked on applications of ASR in music. -->
<!-- <p>My goal as a researcher is to build evidence-based technologies to improve people's lives. My research interests include designing generative models for creative and assistive applications, and music information retrieval.</p> -->
<p>I have founded the company, <a href="https://musigpro.com/" target="_blank">MuSigPro</a> which is a music tech company that aims to provide solutions to bring the power of music closer to people. Through this music technology start-up, we commercialized two AI-based music technologies developed during my PhD and post-doc - a singing quality assessment algorithm, and an audio-to-lyrics time aligner.
	<!-- It has a singing competition platform with an AI-driven singing quality evaluation technology, that was designed during my PhD, and an automatic music-to-lyrics aligner that was designed during my first PostDoc.-->
	</p>
<br><p><a href="https://chitralekha18.github.io/home/papers/CG_ResumeDec2024.pdf" target="_blank" class="btn btn-primary btn-learn"><b>Download CV</b><i class="icon-download4"></i></a> <a href="https://scholar.google.com/citations?user=NFi7pkcAAAAJ&hl=en" target="_blank" class="btn btn-primary btn-learn"><b>Google Scholar</b><i class="https://scholar.google.com/favicon.ico"></i></a> <a href="https://github.com/chitralekha18" target="_blank" class="btn btn-primary btn-learn"><b>Github</b><i class=""></i></a></p><br>

									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<!-- <section class="colorlib-experience" data-section="news">
				<div class="row">
					<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
						<h2>&emsp;&nbsp;&nbsp;News</h2>

						</div>
						</div>


				<div class="colorlib-narrow-content"  style="height:150vh; overflow-y:auto;">
					<div class="row">
						<div class="col-md-12">
							
							<article class="timeline-entry animate-box" data-animate-effect="fadeInLeft">
								<div class="timeline-entry-inner">
						<i class="icon-pen2"></i>&emsp;<b>April 2024:</b> Our paper titled <b>SonicVista: Towards Creating Awareness of Distant Scenes through Sonification</b> accepted in Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2024.<br>
						<i class="icon-pen2"></i>&emsp;<b>April 2024:</b> Our paper titled <a href="https://arxiv.org/abs/2308.11859"><b>Example-Based Framework for Perceptually Guided Audio Texture Generation</b></a> accepted in IEEE/ACM TASLP 2024.<br>

						<i class="icon-pen2"></i>&emsp;<b>March 2023:</b> <font color="red"> <strong>Promotion! </strong></font>🎉 I am happy to share that I got promoted to <b>Senior Research Fellow</b> position, School of Computing, NUS.<br>
						<i class="icon-pen2"></i>&emsp;<b>July 2022:</b> My first field overview paper, titled <a href="https://ieeexplore.ieee.org/document/9829265"><b>Deep Learning Approaches in Topics of Singing Information Processing</b></a> is now published in IEEE/ACM Transactions on Audio, Speech, and Language Processing 2022. It was an enriching experience to collaborate with Dr. Masataka Goto and Prof. Haizhou Li for this paper.<br>
					
				</div>
				</article>

				</div>
				</div>
			
			</section>

 -->






            <!-- <section class="colorlib-services" data-section="publications"> -->
				<section data-section="publications"></section>
				<div class="row" >
					<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
<!--							<span class="heading-meta">What I do?</span>-->
						<h2>&emsp;&nbsp;&nbsp;Publications</h2>
					</div>
				</div>
				<div class="colorlib-narrow-content"  style="height:200vh; overflow-y:auto;">
                    <ol  style="margin-left: 6px; margin-right: 5px" >
						<h3>2025</h3>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/2408.07260"></a>MorphFader: Enabling Fine-grained Controllable Morphing with Text-to-Audio Models<br>
								Purnima Kamath, <strong>Chitralekha Gupta</strong>, and Suranga Nanayakkara<br>
								<span style="font-style: italic;">Proceedings of ICASSP 2025.</span>
								<br>
								
								</div>
						</li>
						<br>
						<h3>2024</h3>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://dl.acm.org/doi/10.1145/3659609"></a>SonicVista: Towards Creating Awareness of Distant Scenes through Sonification<br>
								<strong>Chitralekha Gupta</strong>, Shreyas Sridhar, Denys Matthies, Christophe Jouffrais, and Suranga Nanayakkara<br>
								<span style="font-style: italic;">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2024.</span>
								<br>
								
								</div>
						</li>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/2308.11859">Example-Based Framework for Perceptually Guided Audio Texture Generation</a><br>
								Purnima Kamath, <strong>Chitralekha Gupta</strong>, Lonce Wyse, and Suranga Nanayakkara<br>
								<span style="font-style: italic;">IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024.</span>
								<br>
								
								</div>
						</li>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/2306.03381">VR.net: A Real-world Dataset for Virtual Reality Motion Sickness Research</a> <font color="red"> <strong>- Best Paper Award 🏆</strong></font><br>
								Elliot Wen, <strong>Chitralekha Gupta</strong>, Prasanth Sasikumar, Mark Billinghurst, James Wilmott, Emily Skow, Arindam Dey, and Suranga Nanayakkara<br>
								<span style="font-style: italic;">IEEE VR, 2024.</span>
								<br>
								
								</div>
						</li>
						<br>
						<h3>2023</h3>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span>EMO-KNOW: A Large Scale Dataset on Emotion-Cause<br>
								Mia Nguyen, Yasith Samaradivakara, Prasanth Sasikumar, <b>Chitralekha Gupta</b>, and Suranga Nanayakkara<br>
								<span style="font-style: italic;">Findings of the Association for Computational Linguistics: EMNLP 2023.</span>
								<br>
								
								</div>
						</li>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2286090">Can AI Models Summarize Your Diary Entries? Investigating Utility of Abstractive Summarization for Autobiographical Text</a><br>
								<strong>Chitralekha Gupta*</strong>, Shamane Siriwardhana*, Tharindu Kaluarachchi, Vipula Dissanayake, Suveen Ellawela, and Suranga Nanayakkara<br>
								<span style="font-style: italic;"> International Journal of Human–Computer Interaction (IJHCI), 2023.</span>
								<br>
								
								</div>
						</li>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/2304.11648">Towards Controllable Audio Texture Morphing</a><br>
									<strong>Chitralekha Gupta</strong>*, Purnima Kamath*, Yize Wei, Zhuoyao Li, Suranga Nanayakkara, and Lonce Wyse*<br>
								<span style="font-style: italic;">in ICASSP, 2023.</span>
								<br>
								
								</div>
						</li>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://dl.acm.org/doi/10.1145/3581641.3584083">Evaluating Descriptive Quality of AI-Generated Audio Using Image-Schemas</a><br>
									Purnima Kamath, Zhuoyao Li, <strong>Chitralekha Gupta</strong>, Suranga Nanayakkara, and Lonce Wyse<br>
								<span style="font-style: italic;">in ACM IUI, 2023.</span>
								<br>
								
								</div>
						</li>
						<br>
						<h3>2022</h3>
						<li>
							<div>
								<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="">Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures</a><br>
									<strong>Chitralekha Gupta</strong>, Yize Wei, Zequn Gong, Purnima Kamath, Zhuoyao Li, and Lonce Wyse<br>
								<span style="font-style: italic;">in ISMIR, 2022.</span>
								<br>
								
								</div>
						</li>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://ieeexplore.ieee.org/document/9829265">Deep Learning Approaches in Topics of Singing Information Processing (Overview Paper)</a><br>
							<strong>Chitralekha Gupta</strong>, Haizhou Li, and Masataka Goto<br>
						<span style="font-style: italic;">IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022.</span>
						<br>
						
						</div>
				</li>
				
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://ieeexplore.ieee.org/document/9833328">Automatic Lyrics Transcription of Polyphonic Music With Lyrics-Chord Multi-Task Learning</a><br>
							Xiaoxue Gao, <strong>Chitralekha Gupta</strong>, and Haizhou Li<br>
						<span style="font-style: italic;">IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022.</span>
						<br>
						
						</div>
				</li>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/pdf/2204.03307.pdf">Genre-conditioned Acoustic Models for Automatic Lyrics Transcription of Polyphonic Music</a><br>
							Xiaoxue Gao, <strong>Chitralekha Gupta</strong>, and Haizhou Li<br>
						<span style="font-style: italic;">in ICASSP 2022.</span>
						<br>
						
						</div>
				</li>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/pdf/2204.03306.pdf">Music-robust Automatic Lyrics Transcription of Polyphonic Music</a><br>
							Xiaoxue Gao, <strong>Chitralekha Gupta</strong>, and Haizhou Li<br>
						<span style="font-style: italic;">in SMC 2022.</span>
						<br>
						
						</div>
				</li>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/2206.13085">Sound Model Factory: An Integrated System Architecture for Generative Audio Modelling</a><br>
							Lonce Wyse, Purnima Kamath, and <strong>Chitralekha Gupta</strong><br>
						<span style="font-style: italic;">in Springer LNCS, EvoMusART 2022.</span>
						<br>
						
						</div>
				</li>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/2207.07336">PoLyScribers: Joint Training of Vocal Extractor and Lyrics Transcriber for Polyphonic Music</a><br>
							Xiaoxue Gao, <strong>Chitralekha Gupta</strong>, and Haizhou Li<br>
						<span style="font-style: italic;">submitted to TASLP 2022.</span>
						<br>
						
						</div>
				</li>
				<br>
				<h3>2021</h3>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://ieeexplore.ieee.org/document/9689271">Training Explainable Singing Quality Assessment Network with Augmented Data</a><br>
						 Jinhu Li, <strong>Chitralekha Gupta</strong>, and Haizhou Li<br>
						<span style="font-style: italic;">in proceedings of APSIPA 2021.</span>
						<br>
					 
						</div>
				</li>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://ieeexplore.ieee.org/document/9689470/">Towards Reference-Independent Rhythm Assessment of Solo Singing</a><br>
						 <strong>Chitralekha Gupta</strong>, Jinhu Li, and Haizhou Li<br>
						<span style="font-style: italic;">in proceedings of APSIPA 2021.</span>
						<br>
					 
						</div>
				</li>
				<li>
					<div>
						<span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/2103.07390">Signal Representations for Synthesizing Audio Textures with Generative Adversarial Networks</a><br>
						 <strong>Chitralekha Gupta</strong>, Purnima Kamath, and Lonce Wyse<br>
						<span style="font-style: italic;">in proceedings of SMC 2021.</span>
						<br>
					 
						</div>
				</li>
				<br>
				<h3>2020</h3>
			    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="">Spectral Features and Pitch Histogram for Automatic Singing Quality Evaluation with CRNN</a><br>
							 Huang Lin, <strong>Chitralekha Gupta</strong>, and Haizhou Li<br>
							<span style="font-style: italic;">in proceedings of APSIPA 2020.</span>
						    <br>
						 
							</div>
					</li>
			    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://drive.google.com/file/d/1TloXFrwviOLHNLG9XEiVWQarPEhiHY0t/view?usp=sharing">Automatic Rank Ordering of Singing Vocals with Twin Neural Network</a><br>
							 <strong>Chitralekha Gupta</strong>, Huang Lin, and Haizhou Li<br>
							<span style="font-style: italic;">in Proceedings of ISMIR 2020.</span>
						    <br>
						 
							</div>
					</li>
                        <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://ieeexplore.ieee.org/document/8871113">Automatic Leaderboard: Evaluation of Singing Quality without a Standard Reference</a><br>
							 <strong>Chitralekha Gupta</strong>, Haizhou Li, and Ye Wang<br>
							<span style="font-style: italic;">IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2020</span>
						    <br>
						 
							</div>
					</li>
                        <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/1909.10200">Automatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?</a><br>
							 <strong>Chitralekha Gupta</strong>, Emre Yilmaz, and Haizhou Li<br>
							<span style="font-style: italic;">In Proceddings of ICASSP 2020.</span>
						    <br>
						 
							</div>
					</li>
					<br>
					<h3>2019</h3>

		<li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://arxiv.org/abs/1906.10369">Acoustic Modeling for Automatic Lyrics-to-Audio Alignment</a><br>
							 <strong>Chitralekha Gupta</strong>, Emre Yilmaz, and Haizhou Li<br>
							<span style="font-style: italic;">In Proceedings of Interspeech 2019</span>
						    <br>
						 
							</div>
					</li>
		<li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://ieeexplore.ieee.org/document/8682582">Automatic Lyrics-to-Audio Alignment on Polyphonic Music using Singing-Adapted Acoustic Models</a><br>
							 <strong>Chitralekha Gupta</strong>*, Bidisha Sharma*, Haizhou Li, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of ICASSP 2019 (*equal contributors)</span>
						    <br>
						 
							</div>
					</li>
					<br>
					<h3>2018</h3>
                <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="papers/APSIPA2018automatic-evaluation-singing.pdf">Automatic Evaluation of Singing Quality without a Reference</a><br>
							 <strong>Chitralekha Gupta</strong>, Haizhou Li, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of APSIPA ASC 2018</span>
						    <br>
						 
							</div>
					</li>
                <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.cambridge.org/core/journals/apsipa-transactions-on-signal-and-information-processing/article/technical-framework-for-automatic-perceptual-evaluation-of-singing-quality/5F6AECB907FE842481D070850EDF1EFA">A Technical Framework for Automatic Perceptual Evaluation of Singing Quality</a><br>
							 <strong>Chitralekha Gupta</strong>, Haizhou Li, and Ye Wang<br>
							<span style="font-style: italic;">Transactions of APSIPA 2018</span>
						    <br>
						 
							</div>
					</li>
                    
					<li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="http://ismir2018.ircam.fr/doc/pdfs/30_Paper.pdf">Semi-supervised lyrics and solo-singing alignment</a><br>
							 <strong>Chitralekha Gupta</strong>, Rong Tong, Haizhou Li, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of ISMIR 2018</span>
						    <br>
						 
							</div>
					</li>
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.smcnus.org/wp-content/uploads/2018/07/Chitralekha_Interspeech2018.pdf">Automatic Pronunciation Evaluation of Singing</a><br>
							 <strong>Chitralekha Gupta</strong>, Haizhou Li, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of Interspeech 2018</span>
						    <br>
						 
							</div>
					</li>
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="http://ismir2018.ircam.fr/doc/pdfs/117_Paper.pdf">Empirically weighing the importance of decision factors when selecting music to sing.</a><br>
							 Michael Mustaine, Karim Ibhrahim, <strong>Chitralekha Gupta</strong>, and Ye Wang<br>
							<span style="font-style: italic;">Accepted for ISMIR 2018</span>
						    <br>
						 
							</div>
					</li>
					<br>
					<h3>2017</h3>
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.smcnus.org/wp-content/uploads/2013/09/WP-P2.5.pdf">Perceptual Evaluation of Singing Quality</a> <font color="red"> <strong>- Best Student Paper Award 🏆</strong></font><br>
							 <strong>Chitralekha Gupta</strong>, Haizhou Li, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of Asia-Pacific Signal and Information Processing Association (APSIPA), Kuala Lumpur, Dec. 2017.</span>
						    <br>
						 
							</div>
					</li>
                    
                    
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.smcnus.org/wp-content/uploads/2013/09/invitedtalk-01.pdf">Using Music Technology to Motivate Foreign Language Learning</a><br>
							 Douglas Turnbull, <strong>Chitralekha Gupta</strong>, Dania Murad, Michael Barone, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of International Conference on Orange Technologies, ICOT, Singapore, Dec. 2017</span>
						    <br>
						 
							</div>
					</li>
                        
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.smcnus.org/wp-content/uploads/2017/08/Gupta2017.pdf">Towards automatic mispronunciation detection in singing</a><br>
							 <strong>Chitralekha Gupta</strong>, David Grunberg, Preeti Rao, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of International Society of Music Information Retrieval (ISMIR), Suzhou, Oct. 2017</span>
						    <br>
						 
							</div>
					</li>
                        
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.smcnus.org/wp-content/uploads/2017/08/Ibrahim2017.pdf">Intelligibility of Sung Lyrics: A Pilot Study</a><br>
							 Karim Magdi, David Grunberg, Kat Agres, <strong>Chitralekha Gupta</strong>, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of International Society of Music Information Retrieval (ISMIR), Suzhou, Oct. 2017</span>
						    <br>
						 
							</div>
					</li>

                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.smcnus.org/wp-content/uploads/2013/09/Duan2017.pdf">SECCIMA: Singing and Ear Training for Children with Cochlear Implants via a Mobile Application</a><br>
							 Zhiyan Duan, <strong>Chitralekha Gupta</strong>, Graham Percival, David Grunberg, and Ye Wang<br>
							<span style="font-style: italic;">In Proceedings of Sound and Music Computing (SMC), Helsinki, July 2017</span>
						    <br>
						 
							</div>
					</li>
					<br>
					<h3>Earlier Publications</h3> 
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="http://www.radarindia.com/irsi13papers/13-fp-220.pdf">Spectral Estimation of Clutter for Matched Illumination</a><br>
							 <strong>Chitralekha Gupta</strong>, Kaushal Jadia, Avik Santra, and Rajan Srinivasan<br>
							<span style="font-style: italic;">In Proceedings of International Radar Symposium India (IRSI)}, Bangalore, Dec. 2013</span>
						    <br>
						 
							</div>
					</li>

                      
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.ee.iitb.ac.in/student/~daplab/publications/2012/cg-pr-lncs2011_v11.pdf">Objective Assessment of Ornamentation in Indian Classical Singing</a><br>
							 <strong>Chitralekha Gupta</strong>, and Preeti Rao<br>
							<span style="font-style: italic;">S. Ystad et al. (Eds.): CMMR/FRSM 2011, Springer Lecture Notes on Computer Science 7172, pp. 1-25, 2012 (Master's thesis work)</span>
						    <br>
						 
							</div>
					</li>
                    

                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.researchgate.net/publication/267706758_AN_OBJECTIVE_ASSESSMENT_TOOL_FOR_ORNAMENTATION_IN_SINGING">An objective evaluation tool for ornamentation in singing</a><br>
							 <strong>Chitralekha Gupta</strong>, and Preeti Rao<br>
							<span style="font-style: italic;">In Proceedings of International Symposium on Computer Music Modelling and Retrieval (CMMR) and Frontiers of Research on Speech and Music (FRSM), Bhubaneswar, India, March 2011</span>
						    <br>
						 
							</div>
					</li>

                        
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.ee.iitb.ac.in/student/~daplab/publications/vr-cg-pr-AMR-11.pdf">Context-aware features for singing voice detection in polyphonic music</a><br>
							 Vishweshwara Rao, <strong>Chitralekha Gupta</strong>, and Preeti Rao<br>
							<span style="font-style: italic;">In 9th International Workshop on Adaptive Multimedia Retrieval, Barcelona, July 2011</span>
						    <br>
						 
							</div>
					</li>
                        
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span><a href="https://www.ee.iitb.ac.in/student/~daplab/publications/ap-cg-pr-ncc10.pdf">Evaluating Vowel Pronunciation Quality: Formant Space Matching versus ASR Confidence Scoring</a><br>
							 Ashish Patil, <strong>Chitralekha Gupta</strong>, and Preeti Rao<br>
							<span style="font-style: italic;">In Proceedings of 16th National Conference on Communications – 2010, IIT Madras, Chennai, Jan. 2010</span>
						    <br>
						 
							</div>
					</li>
                        
<!--
                    <li>
						<div>
                            <span style="font-size:15px; color:black;"><span _fck_bookmark="1" style="display: none;"> </span><span _fck_bookmark="1" style="display: none;">&nbsp;</span>Vista – An Obstacle Detector for the Visually Challenged<br>
							 <strong>Chitralekha Gupta</strong>, Snehal Raj, and M. Gosavi<br>
							<span style="font-style: italic;">In Proceedings of National Conference on Recent Advances in Electrical Machines and Energy Systems}, Charotar Institute of Technology, Changa, June 2008 (Bachelor's thesis work)</span>
						    <br>
						 
							</div>
					</li>
					
-->
                    </ol>
                    
				</div>
			</section>
            
            
			<section class="colorlib-experience" data-section="ongoing projects">
				<div class="row">
					<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
						<h2>&emsp;&nbsp;&nbsp;Ongoing Projects</h2>

						</div>
						</div>

						<!-- <div class="square"> -->
				<div class="colorlib-narrow-content"  style="height:150vh; overflow-y:auto;">
					<div class="row">
						<div class="col-md-12">
							<div class="square">
							<article class="timeline-entry animate-box" data-animate-effect="fadeInLeft">
								<div class="timeline-entry-inner">
						&emsp;<h3>&emsp;Sonic Vista</h3> 
						
						<div class="container">
							<div class="one-half">We are building a wearable artifact for people with visual impairments, called SonicVista, that can provide information and experience of distant surrounding environmental scenes (that are beyond audible range) through generative sounds.
								We are exploring the use of Meta's ARIA glasses in this project.</div>
							<div class="other-half"><img src="images/sonicvista1.jpg"></div>
							
						</div><br>
						&emsp;<i class="icon-pen2"></i>&emsp;<b>April 2024:</b> Our paper titled <b>SonicVista: Towards Creating Awareness of Distant Scenes through Sonification</b> accepted in Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2024.<br>
						&emsp;<i class="icon-pen2"></i>&emsp;<b>March 2024:</b> I and my colleague Shreyas were invited to present our work at <b>Meta's ARIA Summit</b> in Redmond WA, USA.<br>
				</div>
				<br>
				</article>
			</div>
				</div>
				</div>
				<br><br>

				<div class="row">
					<div class="col-md-12">
						<div class="square">
						<article class="timeline-entry animate-box" data-animate-effect="fadeInLeft">
							<div class="timeline-entry-inner">
								&emsp;<h3>&emsp;Controlled Audio Gen</h3> 
					
					<div class="container">
						<div class="one-half">Video games and movies often require custom sound effects. We are investigating methods to generate environmental sounds with the ability to control various aspects of these sounds, through supervised as well as unsupervised methods of control. For example, we would like to be able to generate wind sounds at low or high wind strength, or the sound of rain when it drizzles or buckets down. </div>
						<div class="other-half"><img src="images/audiocontrol.png"></div>
						
					</div><br>
					&emsp;<i class="icon-pen2"></i>&emsp;<b>April 2024:</b> Our paper titled <b>Example-Based Framework for Perceptually Guided Audio Texture Generation</b> is accepted in IEEE/ACM Transactions of Audio, Speech, and Language Processing, 2024.<br>
					&emsp;<i class="icon-pen2"></i>&emsp;<b>Previous Publications: </b> <br>
					<ul>
						<li><a href="https://arxiv.org/abs/2304.11648">Towards Controllable Audio Texture Morphing</a>, ICASSP 2023</li>
						<li><a href="https://dl.acm.org/doi/10.1145/3581641.3584083">Evaluating Descriptive Quality of AI-Generated Audio Using Image-Schemas</a>, ACM IUI 2023.</li>
						<li><a href="https://arxiv.org/abs/2208.10743">Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures</a>, ISMIR 2022.</li>
						<li><a href="https://arxiv.org/abs/2206.13085">Sound Model Factory: An Integrated System Architecture for Generative Audio Modelling</a>, Springer LNCS, EvoMusART 2022.</li>
						<li><a href="https://arxiv.org/abs/2103.07390">Signal Representations for Synthesizing Audio Textures with Generative Adversarial Networks</a>, SMC 2021.</li>
					</ul>
					<br>
			</div>
			
			</article></div>

			</div>
			</div>
				</div>
				</section>
            
            
            
			<!-- <section class="colorlib-services" data-section="demos"> -->
				<section data-section="demos"></section>
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3 animate-box" data-animate-effect="fadeInLeft">
<!--							<span class="heading-meta">What I do?</span>-->
							<h2>Demo videos</h2>
						</div>
					</div>
					<div class="row row-pt-md">
						<div class="col-md-6 text-center animate-box">
							<div class="services color-1">
								<span class="icon">
									<i class="icon-phone3"></i>
								</span>
								<div class="desc"  style="height:70vh; overflow-y:auto;">
                                    <iframe width="320" height="160" src="https://www.youtube.com/embed/E0wwwpxaUOM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                     <h3>MuSigPro</h3>
									<h3>MuSigPro Product Demo</h3>
                                    <p align="left">MuSigPro's singing karaoke app is now available on Google Playstore</p>                                    
                                   <p align="left">Try it out now! -- <a href="https://play.google.com/store/apps/details?id=com.musigpro.app" target="_blank">Download App</a></p>
								</div>
							</div>
						</div>
						
					
						<div class="col-md-6 text-center animate-box">
							<div class="services color-2" >
								<span class="icon">
									<i class="icon-bulb"></i>
								</span>
								<div class="desc"  style="height:70vh; overflow-y:auto;">
                                    <iframe width="320" height="160" src="https://www.youtube.com/embed/IAlsECqd9IE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                     <h3>MuSigPro</h3>
									<h3>Automatic Leaderboard Generation of Singers using Reference-Independent Singing Quality Evaluation Methods</h3>
                                    <p align="left">This technology has been awarded the NUS Graduate Research Innovation Program (GRIP) start-up grant, to establish the company MuSigPro Pte. Ltd. </p>
<!--                                     <p align="left">C. Gupta, H. Li and Y. Wang, <a href="https://ieeexplore.ieee.org/document/8871113" target="_blank">"Automatic Leaderboard: Evaluation of Singing Quality Without a Standard Reference,"</a> in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 13-26, 2020.</p> -->
                                    
                                   <p align="left">Try it out yourself! -- <a href="https://musigpro.com" target="_blank">https://musigpro.com</a></p>
								</div>
							</div>
						</div>
                        <div class="col-md-6 text-center animate-box">
							<div class="services color-3">
								<span class="icon">
									<i class="icon-data"></i>
								</span>
								<div class="desc"  style="height:70vh; overflow-y:auto;">
                                    <iframe src="https://drive.google.com/file/d/1oGdXQ9d3SfecPu8R3TBhY8kufFfXsd8_/preview" width="320" height="160"></iframe>           
<!--                                    <iframe width="560" height="315" src="https://www.youtube.com/embed/z-iDr8WJUH4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
									<h3>AutoLyrixAlign</h3>
									<h3>Automatic lyrics-to-audio alignment system for polyphonic music audio</h3>
                                    <p>Demo submitted to ICASSP 2020 Show and Tell</p>
                                    <p align="left">This system has outperformed all other systems in the International Music Information Retrieval Evaluation eXchange platform MIREX 2019, with mean absolute word alignment error of less than 200 ms across all test datasets (<a href="https://www.music-ir.org/mirex/wiki/2019:Automatic_Lyrics-to-Audio_Alignment_Results">Mirex Results</a>)</p>
<!--                                     <p align="left">C. Gupta, E. Yılmaz, H. Li, <a href="https://arxiv.org/abs/1909.10200" target="_blank">"Automatic Lyrics Alignment and Transcription in Polyphonic Music: Does Background Music Help?,"</a> arXiv preprint:1909.10200v2 [eess.AS], 2019 (submitted to ICASSP 2020).</p> -->
                                    <p align="left">Try it out yourself! -- <a href="https://autolyrixalign.hltnus.org" target="_blank">https://autolyrixalign.hltnus.org</a></p>
								</div>
							</div>
						</div>
						<div class="col-md-6 text-center animate-box" >
							<div class="services color-4">
								<span class="icon">
									<i class="icon-phone3"></i>
								</span>
								<div class="desc"  style="height:70vh; overflow-y:auto;">
<!--                                    <iframe src="https://drive.google.com/file/d/1RlWDe9OnKDASOvdJmbx63BsHliQWVT2C/preview" width="640" height="480"></iframe>-->           <iframe width="320" height="160" src="https://www.youtube.com/embed/zjtNUbo-v7w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
									<h3>Speak-to-Sing</h3>
									<h3>A Personalized Speech-to-Singing Conversion System</h3>
                                    <p align="left">Presented at Interspeech 2019 Show and Tell, Graz Austria <a href="https://drive.google.com/open?id=1KKURwKQAQbEOPwufHOkrBFguVQk-ElOg" target="_blank">Poster link</a></p>
                                    <p align="left">Try it out yourself! -- <a href="https://speak-to-sing.hltnus.org/" target="_blank">https://speak-to-sing.hltnus.org</a></p>
								</div>
							</div>
						</div>
				
					</div>
				</div>
			<!-- </section> -->
	

		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>

